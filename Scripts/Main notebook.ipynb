{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f2b224-649c-4b37-a52c-5fc578970df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r /home/onyxia/work/python_ensae/requirements.txt -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO #permet de stocker en mémoire\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Imports pour l'ACP\n",
    "from sklearn.preprocessing import MinMaxScaler #pour normaliser entre 0 et 1\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Imports pour la modélisation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca17b36b-90fb-4f29-af6d-c849e251a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url2019 = \"https://www.insee.fr/fr/statistiques/fichier/4809583/fd_eec19_csv.zip\" #enquete 2019\n",
    "url2020=\"https://www.insee.fr/fr/statistiques/fichier/5393560/fd_eec20_csv.zip\" #enquête 2020 en exemple\n",
    "\n",
    "# Télécharge le fichier ZIP\n",
    "requete = requests.get(url2019)\n",
    "zip_df = ZipFile(BytesIO(requete.content)) #créer un fichier ZIP\n",
    "\n",
    "# Extraire le fichier CSV du ZIP\n",
    "with zip_df.open(zip_df.namelist()[0]) as extrait:\n",
    "    EEC_2019 = pd.read_csv(extrait, delimiter=\";\") # Lire le fichier CSV avec pandas\n",
    "\n",
    "# Télécharge le fichier ZIP\n",
    "requete = requests.get(url2020)\n",
    "zip_df = ZipFile(BytesIO(requete.content)) #créer un fichier ZIP\n",
    "\n",
    "# Extraire le fichier CSV du ZIP\n",
    "with zip_df.open(zip_df.namelist()[0]) as extrait:\n",
    "    EEC_2020 = pd.read_csv(extrait, delimiter=\";\") # Lire le fichier CSV avec pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d5a7f-5a8a-4c20-a9eb-a2989868e719",
   "metadata": {},
   "source": [
    "# I - Statistique descriptives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5882922b-590e-49db-bcf2-1207a656f9e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## A - Statut d'activité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691d5e3-e4e4-4f86-8157-ee79c1e8afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Catégories possibles\n",
    "categories = [\"Actif occupé\", \"Chômeur\", \"Inactif\"]\n",
    "value = [1,2,3] #on a aussi des NA mais ils ne sont pas pondérés\n",
    "N=EEC_2019[\"EXTRIAN\"].sum()\n",
    "\n",
    "#Effectif par catégorie\n",
    "result_dict = {\"ACTEU\": categories, 'Effectif': [], 'Proportion':[]}  #à chaque valeur on associe la somme des occurences pondérées\n",
    "for val in value :\n",
    "    somme_pond = EEC_2019[EEC_2019[\"ACTEU\"]==val][\"EXTRIAN\"].sum()\n",
    "    result_dict['Effectif'].append(round(somme_pond,0))\n",
    "    result_dict['Proportion'].append(round(somme_pond/N,3))\n",
    "data = pd.DataFrame(result_dict).sort_values(by=\"ACTEU\").reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a0ef3-a709-4f47-87f1-80d45a4147a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## B - Durée du chômage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffdcfa3-3355-415c-aac0-7fa3c1d468dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#répartiton de la durée de chômage avec la variable ANCCHOM (on pondère par EXTRIAN)\n",
    "EEC_chom_2019=EEC_2019[EEC_2019[\"ACTEU\"]==2]\n",
    "EEC_chom_2020=EEC_2020[EEC_2020[\"ACTEU\"]==2]\n",
    "EEC_chom_2019[\"ANCCHOM\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad33b55c-5e38-4cca-a78d-79f2c7f97b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([EEC_chom_2019[\"ANCCHOM\"],EEC_chom_2020[\"ANCCHOM\"]],bins=[i - 0.5 for i in range(1, 10)], weights = [EEC_chom_2019[\"EXTRIAN\"],EEC_chom_2020[\"EXTRIAN\"]], align='mid',width=0.4, density=True,label=[\"2019\",\"2020\"])\n",
    "plt.title('Histogramme de la durée du chômage')\n",
    "plt.xticks(range(1,10), [\"< 1 mo\",\"1< <3mo\",\"3< <6mo\",\"6< <12 mo\",\"12< <18 mo\",\"18 mo< <2 y\",\"2< <3 y\",\">3 y\",\"\"], rotation=45, ha=\"right\")\n",
    "plt.xlabel('Durée du chômage')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d0776e-be06-4d8e-8d98-9642401c0e0a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## C- CSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5daf0cf-501e-423c-b64a-c7589dabefad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Catégories possibles\n",
    "categories = [\"Non-renseigné\", \"Agriculteurs\", \"Artisans, commerçants, chefs d'entreprise\", \"Cadres et professions intellectuelles supérieures\", \"Professions intermédiaires\", \"Employés\", \"Ouvriers\", \"Inactifs ayant déjà eu une activité pro\", \"Chômeurs et inactifs n'ayant jamais travaillé\", \"NA\"]\n",
    "value = EEC_2019[\"CSTOTR\"].unique()\n",
    "\n",
    "#tableau avec CSTOR/Effectif en emploi/Effectif chômeur tout cela pondéré par EXTRIAN\n",
    "result_dict = {\"CSTOTR\": value, 'Effectif_emploi': [], 'Effectif_chom':[]}  #à chaque valeur on associe la somme des occurences pondérées\n",
    "for val in value :\n",
    "    somme_pond_E = EEC_2019[(EEC_2019[\"CSTOTR\"] == val) & (EEC_2019[\"ACTEU\"]==1)][\"EXTRIAN\"].sum()\n",
    "    somme_pond_C = EEC_2019[(EEC_2019[\"CSTOTR\"] == val)& (EEC_2019[\"ACTEU\"]==2)][\"EXTRIAN\"].sum()\n",
    "    result_dict['Effectif_emploi'].append(round(somme_pond_E,0))\n",
    "    result_dict['Effectif_chom'].append(round(somme_pond_C,0))\n",
    "data = pd.DataFrame(result_dict).sort_values(by=\"CSTOTR\").reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fac973e-9314-4ba7-ba4b-9f54ec60424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(data[\"CSTOTR\"],data[\"Effectif_chom\"], color='orange', label='Chômeurs')\n",
    "plt.bar(data[\"CSTOTR\"],data[\"Effectif_emploi\"], color='blue', label='Au travail', bottom=data[\"Effectif_chom\"])\n",
    "plt.title('Fréquence du chômage en fonction de la catégorie socioprofessionnelle')\n",
    "plt.xlabel('Catégorie socioprofessionnelle')\n",
    "plt.ylabel('Effectif')\n",
    "plt.xticks(data[\"CSTOTR\"],categories, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c198e-27cb-4b40-a6b4-c26f7f3cc963",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## D - Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0680a8d4-6fc0-4d52-b1c9-c0c1ce5f5620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chômage par âge\n",
    "#Catégories possibles\n",
    "categories = [\"15-29\",\"30-39\",\"40-49\",\"50-59\",\"60+\",]\n",
    "value = EEC_2019[\"AGE5\"].unique()\n",
    "\n",
    "#tableau avec AGE/Effectif en emploi/Effectif chômeur tout cela pondéré par EXTRIAN\n",
    "result_dict = {\"AGE5\": categories, 'Effectif_emploi': [], 'Effectif_chom':[], 'Proportion_chom':[], 'Effectif_inac':[], 'Proportion_inac':[]}  #à chaque valeur on associe la somme des occurences pondérées\n",
    "for val in value :\n",
    "    somme_pond_E = EEC_2019[(EEC_2019[\"AGE5\"] == val) & (EEC_2019[\"ACTEU\"]==1)][\"EXTRIAN\"].sum()\n",
    "    somme_pond_C = EEC_2019[(EEC_2019[\"AGE5\"] == val)& (EEC_2019[\"ACTEU\"]==2)][\"EXTRIAN\"].sum()\n",
    "    somme_pond_I = EEC_2019[(EEC_2019[\"AGE5\"] == val)& (EEC_2019[\"ACTEU\"]==3)][\"EXTRIAN\"].sum()\n",
    "    result_dict['Effectif_emploi'].append(round(somme_pond_E,0))\n",
    "    result_dict['Effectif_chom'].append(round(somme_pond_C,0))\n",
    "    result_dict['Proportion_chom'].append(round(somme_pond_C/(somme_pond_C + somme_pond_E+somme_pond_I),2))\n",
    "    result_dict['Effectif_inac'].append(round(somme_pond_I,0))\n",
    "    result_dict['Proportion_inac'].append(round(somme_pond_I/(somme_pond_I + somme_pond_E+somme_pond_C),2))\n",
    "data = pd.DataFrame(result_dict).sort_values(by=\"AGE5\").reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4ddcc-60c1-4642-992a-5ba10581ffa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(data[\"AGE5\"],data[\"Effectif_emploi\"], color='blue', label='Au travail')\n",
    "plt.bar(data[\"AGE5\"],data[\"Effectif_chom\"], color='orange', label='Chômeurs', bottom=data[\"Effectif_emploi\"])\n",
    "plt.bar(data[\"AGE5\"],data[\"Effectif_inac\"], color='red', label='Inactifs', bottom=data[\"Effectif_emploi\"] + data[\"Effectif_chom\"])\n",
    "plt.title(\"Statut d'activité en fonction de l'âge\")\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Effectif')\n",
    "plt.xticks(data[\"AGE5\"],categories, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb94762-0ed5-421d-bd3e-4560e564d92f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## E - Sexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4e3b0-6582-454a-9663-d16e82d16c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chômage et inactivité par sexe\n",
    "#Catégories possibles\n",
    "categories = [\"Homme\",\"Femme\"]\n",
    "value = EEC_2019[\"SEXE\"].unique()\n",
    "\n",
    "#tableau avec SEXE/Effectif en emploi/Effectif chômeur et pour inactifs tout cela pondéré par EXTRIAN\n",
    "result_dict = {\"SEXE\": categories, 'Effectif_emploi': [], 'Effectif_chom':[], 'Proportion_chom':[], 'Effectif_inac':[], 'Proportion_inac':[]}  #à chaque valeur on associe la somme des occurences pondérées\n",
    "for val in value :\n",
    "    somme_pond_E = EEC_2019[(EEC_2019[\"SEXE\"] == val) & (EEC_2019[\"ACTEU\"]==1)][\"EXTRIAN\"].sum()\n",
    "    somme_pond_C = EEC_2019[(EEC_2019[\"SEXE\"] == val)& (EEC_2019[\"ACTEU\"]==2)][\"EXTRIAN\"].sum()\n",
    "    somme_pond_I = EEC_2019[(EEC_2019[\"SEXE\"] == val)& (EEC_2019[\"ACTEU\"]==3)][\"EXTRIAN\"].sum()\n",
    "    result_dict['Effectif_emploi'].append(round(somme_pond_E,0))\n",
    "    result_dict['Effectif_chom'].append(round(somme_pond_C,0))\n",
    "    result_dict['Proportion_chom'].append(round(somme_pond_C/(somme_pond_C + somme_pond_E+somme_pond_I),2))\n",
    "    result_dict['Effectif_inac'].append(round(somme_pond_I,0))\n",
    "    result_dict['Proportion_inac'].append(round(somme_pond_I/(somme_pond_I + somme_pond_E+somme_pond_C),2))\n",
    "data = pd.DataFrame(result_dict).sort_values(by=\"SEXE\").reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ea504-9fa5-4b34-855c-d4dab7f2f22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(data[\"SEXE\"],data[\"Effectif_emploi\"], color='blue', label='Au travail')\n",
    "plt.bar(data[\"SEXE\"],data[\"Effectif_chom\"], color='orange', label='Chômeurs', bottom=data[\"Effectif_emploi\"])\n",
    "plt.bar(data[\"SEXE\"],data[\"Effectif_inac\"], color='red', label='Inactifs', bottom=data[\"Effectif_emploi\"] + data[\"Effectif_chom\"])\n",
    "plt.title(\"Statut d'activité en fonction du sexe\")\n",
    "plt.xlabel('SEXE')\n",
    "plt.ylabel('Effectif')\n",
    "plt.xticks(data[\"SEXE\"],categories, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7dd05a-65a3-4e72-be89-d60555d6a85a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## F - Catégorie de commune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a751c32-25c5-4cb3-b848-f28a6ba8ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chômage et inactivité par taille de la commune\n",
    "#Catégories possibles\n",
    "categories = [\"Grandes aires urbaines\",\"Autres aires\",\"Autres communes multipolarisées\",\"Communes isolées hors influence des pôles\"]\n",
    "value = EEC_2019[\"CATAU2010R\"].unique()\n",
    "\n",
    "#tableau avec CAUTAU2010R/Effectif en emploi/Effectif chômeur et pour inactifs tout cela pondéré par EXTRIAN\n",
    "result_dict = {\"CATAU2010R\": categories, 'Effectif_emploi': [], 'Effectif_chom':[], 'Proportion_chom':[], 'Effectif_inac':[], 'Proportion_inac':[]}  #à chaque valeur on associe la somme des occurences pondérées\n",
    "for val in value :\n",
    "    somme_pond_E = EEC_2019[(EEC_2019[\"CATAU2010R\"] == val) & (EEC_2019[\"ACTEU\"]==1)][\"EXTRIAN\"].sum()\n",
    "    somme_pond_C = EEC_2019[(EEC_2019[\"CATAU2010R\"] == val)& (EEC_2019[\"ACTEU\"]==2)][\"EXTRIAN\"].sum()\n",
    "    somme_pond_I = EEC_2019[(EEC_2019[\"CATAU2010R\"] == val)& (EEC_2019[\"ACTEU\"]==3)][\"EXTRIAN\"].sum()\n",
    "    result_dict['Effectif_emploi'].append(round(somme_pond_E,0))\n",
    "    result_dict['Effectif_chom'].append(round(somme_pond_C,0))\n",
    "    result_dict['Proportion_chom'].append(round(somme_pond_C/(somme_pond_C + somme_pond_E+somme_pond_I),2))\n",
    "    result_dict['Effectif_inac'].append(round(somme_pond_I,0))\n",
    "    result_dict['Proportion_inac'].append(round(somme_pond_I/(somme_pond_I + somme_pond_E+somme_pond_C),2))\n",
    "data = pd.DataFrame(result_dict).reset_index(drop=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cfd1a2-001d-462f-a56a-96567d5833c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(data[\"CATAU2010R\"],data[\"Effectif_emploi\"], color='blue', label='Au travail')\n",
    "plt.bar(data[\"CATAU2010R\"],data[\"Effectif_chom\"], color='orange', label='Chômeurs', bottom=data[\"Effectif_emploi\"])\n",
    "plt.bar(data[\"CATAU2010R\"],data[\"Effectif_inac\"], color='red', label='Inactifs', bottom=data[\"Effectif_emploi\"] + data[\"Effectif_chom\"])\n",
    "plt.title(\"Statut d'activité en fonction du sexe\")\n",
    "plt.xlabel('Catégorie de commune du logement de résidence')\n",
    "plt.ylabel('Effectif')\n",
    "plt.xticks(data[\"CATAU2010R\"],categories, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e69c2d-03d0-4df1-a0b5-2692e52b7a31",
   "metadata": {},
   "source": [
    "# II - Analyse en composantes principales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f650fb-c8f6-4f96-a748-428f3ce921e9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## A - Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec2ce8-55f6-4572-9f7f-e79a38435888",
   "metadata": {},
   "source": [
    "Variables à inclure dans la matrice de variance-covariance : ACTEU, SEXE,DIP11, type de ménages, CSTOTR, NFRRED,NBTOTE (heures travaillées usuellement) HPLUSA (nombre d'heures voulues), implication dans la recherche (CONTACT,DEM),accepterait temps partiel, ANCCHOM, AGE5, CATAU2010R ruralité, COURED ENFRED couple\n",
    "\n",
    "Variables à transformer en indicatrices (attention ne pas mettre toutes les catégories) : ACTEU DIP11, CSTOTR, NFRRED, AGE5, CATAU2010R + plus variables à régler en 0,1\n",
    "\n",
    "Variables à normaliser : NBTOTE, HPLUSA, ANNCHOM (mettre unité)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf1ac5-4fca-4a61-bc19-b4616e000826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'une base PCA\n",
    "EEC_PCA=EEC_2019\n",
    "EEC_PCA=pd.DataFrame(EEC_PCA[[\"EXTRIAN\",\"ACTEU\",\"SEXE\",\"DIP11\",\"CSTOTR\",\"NFRRED\",\"NBTOTE\",\"HPLUSA\",\"HHCE\",\"CONTACT\",\n",
    "\"ANCCHOM\",\"AGE5\",\"AIDFAM\",\"CATAU2010R\",\"COURED\",\"ENFRED\"]]).reset_index(drop=True)\n",
    "EEC_PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352166a5-9fcf-4b30-b335-1d4d1571a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ACTEU\n",
    "EEC_PCA[\"CHOM\"]=0\n",
    "EEC_PCA.loc[EEC_PCA[\"ACTEU\"]==2, \"CHOM\"]=1\n",
    "\n",
    "EEC_PCA[\"INAC\"]=0\n",
    "EEC_PCA.loc[EEC_PCA[\"ACTEU\"]==3, \"INAC\"]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc67e09-5158-461e-a0bf-71aef0d13fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DIP11 que l'on va séparer en indicatrices pour enseignement sup, secondaire et en dessous\n",
    "EEC_PCA[\"infcollege\"]=0 #infcollege = aucun diplome, certif d'études, brevet\n",
    "EEC_PCA.loc[EEC_PCA[\"DIP11\"].isin([71,70,60]), \"infcollege\"]=1\n",
    "\n",
    "EEC_PCA[\"secondaireplus\"]=0 #secondaire plus = secondaire + BTS et DUT...\n",
    "EEC_PCA.loc[EEC_PCA[\"DIP11\"].isin([50,42,41,33,31]), \"secondaireplus\"]=1\n",
    "\n",
    "EEC_PCA[\"sup\"]=0 #le reste\n",
    "EEC_PCA.loc[EEC_PCA[\"DIP11\"].isin([30,11,10]), \"sup\"]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45127707-7a13-4b74-9fe1-b55ba80302b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEXE (0 = H, 1 = F)\n",
    "EEC_PCA.loc[EEC_PCA[\"SEXE\"]==1, \"SEXE_bin\"]=0\n",
    "EEC_PCA.loc[EEC_PCA[\"SEXE\"]==2, \"SEXE_bin\"]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3fc983-7076-44bd-9976-320d0a3a9c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSTOTR : en se basant sur le graphique ci-dessus, je crée une catégorie employé avec les professions intermédiaires, les employés et les ouvriers\n",
    "#et je crée une catégorie employeur avec les cadres sups, artisans, chef d'entreprises\n",
    "EEC_PCA[\"employe\"]=0 \n",
    "EEC_PCA.loc[EEC_PCA[\"CSTOTR\"].isin([4,5,6]), \"employe\"]=1\n",
    "\n",
    "EEC_PCA[\"employeur\"]=0 \n",
    "EEC_PCA.loc[EEC_PCA[\"CSTOTR\"].isin([2,3]), \"employeur\"]=1\n",
    "#on a 3/4 de l'échantillon employe et 1/4 employeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fbb34f-365e-4705-8a4f-18a26352dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NFRRED : variable indicatrice : \"l'individu est-il de nationalité française ?\"\n",
    "EEC_PCA[\"NAT\"]=0\n",
    "EEC_PCA.loc[EEC_PCA[\"NFRRED\"].isin([1,2]), \"NAT\"]=1\n",
    "#1/20 sans nationalité FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07246585-2263-4d6a-aa8e-37568f04b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AGE5 : transformer en deux indicatrices jeunes actifs (0 à 29 ans) ou actifs âgés (plus de 50 ans)\n",
    "EEC_PCA[\"JEUNE\"]=0 \n",
    "EEC_PCA.loc[EEC_PCA[\"AGE5\"].isin([00,15]), \"JEUNE\"]=1\n",
    "\n",
    "EEC_PCA[\"ANCIEN\"]=0 \n",
    "EEC_PCA.loc[EEC_PCA[\"AGE5\"].isin([50,60]), \"ANCIEN\"]=1\n",
    "#1/5 de jeunes et 1/3 d'anciens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b6c214-dc71-4c06-b6f8-c998763530a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONTACT : contact avec pôle emploi depuis dernière enquête (3 mois), 1 si oui\n",
    "EEC_PCA[\"P_E\"]=0\n",
    "EEC_PCA.loc[EEC_PCA[\"CONTACT\"]==1, \"P_E\"]=1\n",
    "#moins d'1/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61968f-684d-4fd7-a986-1ecb544d4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "#création de metrop. 1 si l'individu habite dans un grande aire urbaine : 4/5 échantillon\n",
    "EEC_PCA[\"METROP\"]=0\n",
    "EEC_PCA.loc[EEC_PCA[\"CATAU2010R\"]==1, \"METROP\"]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4f6f3-6b9c-419e-9edd-1d1b8237fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COURED : 1 si l'individu est en couple\n",
    "EEC_PCA[\"COUPLE\"]=0\n",
    "EEC_PCA.loc[EEC_PCA[\"COURED\"]==1, \"COUPLE\"]=1\n",
    "#2/3 en couple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd88dfda-4cd2-4f0a-ad1c-fd39a53f5c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENFRED : 1 si l'individu a un enfant\n",
    "EEC_PCA[\"ENF\"]=0\n",
    "EEC_PCA.loc[EEC_PCA[\"ENFRED\"]==1, \"ENF\"]=1\n",
    "#la moitié a un enfant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4bb9e1-152c-4de5-b8ee-f8f99f1b25f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NBTOTE\n",
    "col = EEC_PCA['NBTOTE']\n",
    "\n",
    "scaler = MinMaxScaler() # initialiser le scaler MinMax\n",
    "\n",
    "col_a_norm = col.values.reshape(-1, 1) # convertir la colonne en une matrice 2D (nécessaire pour le scaler)\n",
    "\n",
    "colonne_normalisee = scaler.fit_transform(col_a_norm)\n",
    "\n",
    "# Remplacer la colonne originale par la colonne normalisée et remplir NaN\n",
    "EEC_PCA['NBTOTE'] = colonne_normalisee\n",
    "EEC_PCA[\"NBTOTE\"].describe()\n",
    "#très peu d'entrées dans cette variable (à remédier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3555a45-9235-4518-b9f6-61b0abf7646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HPLUSA (beaucoup d'informations manquantes, surtout les personnes ne voulant pas modifier leur charge horraire, on va donc prendre \n",
    "#leur charge horraire actuelle comme souhait.\n",
    "EEC_PCA[\"HHCE\"]=EEC_PCA[\"HHCE\"].fillna(0)\n",
    "EEC_PCA.loc[EEC_PCA['HPLUSA'].isna(), \"HPLUSA\"]=EEC_PCA[EEC_PCA['HPLUSA'].isna()][\"HHCE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af24e8d4-76eb-4dbc-bbd4-ccf11983689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = EEC_PCA['HPLUSA']\n",
    "\n",
    "scaler = MinMaxScaler() # initialiser le scaler MinMax\n",
    "\n",
    "col_a_norm = col.values.reshape(-1, 1) # convertir la colonne en une matrice 2D (nécessaire pour le scaler)\n",
    "\n",
    "colonne_normalisee = scaler.fit_transform(col_a_norm)\n",
    "\n",
    "# Remplacer la colonne originale par la colonne normalisée et remplir NaN\n",
    "EEC_PCA['HPLUSA'] = colonne_normalisee\n",
    "EEC_PCA[\"HPLUSA\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e75e65-493d-48c2-874a-0e57acab12ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANCCHOM : recodage en mois\n",
    "EEC_PCA[\"ANCCH\"]=0.0#en emploi\n",
    "EEC_PCA.loc[EEC_PCA[\"ANCCHOM\"]==1, \"ANCCH\"]=1.0\n",
    "EEC_PCA.loc[EEC_PCA[\"ANCCHOM\"]==2, \"ANCCH\"]=2.0\n",
    "EEC_PCA.loc[EEC_PCA[\"ANCCHOM\"]==3, \"ANCCH\"]=4.5\n",
    "EEC_PCA.loc[EEC_PCA[\"ANCCHOM\"]==4, \"ANCCH\"]=9.0\n",
    "EEC_PCA.loc[EEC_PCA[\"ANCCHOM\"]==5, \"ANCCH\"]=15.0\n",
    "EEC_PCA.loc[EEC_PCA[\"ANCCHOM\"]==6, \"ANCCH\"]=21.0\n",
    "EEC_PCA.loc[EEC_PCA[\"ANCCHOM\"]==7, \"ANCCH\"]=30.0\n",
    "EEC_PCA.loc[EEC_PCA[\"ANCCHOM\"]==8, \"ANCCH\"]=40.0\n",
    "EEC_PCA[\"ANCCH\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c927e7e-a649-44ba-8204-330c1f8e8b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = EEC_PCA['ANCCH']\n",
    "\n",
    "scaler = MinMaxScaler() # initialiser le scaler MinMax\n",
    "\n",
    "col_a_norm = col.values.reshape(-1, 1) # convertir la colonne en une matrice 2D (nécessaire pour le scaler)\n",
    "\n",
    "colonne_normalisee = scaler.fit_transform(col_a_norm)\n",
    "\n",
    "# Remplacer la colonne originale par la colonne normalisée et remplir NaN\n",
    "EEC_PCA['ANCCH'] = colonne_normalisee\n",
    "EEC_PCA[\"ANCCH\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842db80-8d3f-4144-b198-190ccc1a6d9a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## B - Matrice de covariances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaedbea-31a5-43dd-a6d8-894df9bd9f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "EEC_PCA1=EEC_PCA[['HPLUSA', 'CHOM','INAC', 'infcollege', 'P_E',\"ANCCH\",\n",
    "       'secondaireplus', 'sup', 'SEXE_bin', 'employe', 'employeur', 'NAT',\n",
    "       'JEUNE', 'ANCIEN', 'METROP', 'COUPLE', 'ENF']] #on omet NBTOTE pas assez d'ind.\n",
    "noms_var=EEC_PCA1.columns\n",
    "EEC_PCAW=EEC_PCA[\"EXTRIAN\"].to_numpy()\n",
    "EEC_PCA1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247523f9-fadd-48a5-ac92-a9e69bb4020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EEC_COV_NP=np.cov(EEC_PCA1.to_numpy(), rowvar=False, aweights=EEC_PCAW)\n",
    "EEC_COV=pd.DataFrame(EEC_COV_NP) #matrice de covariance pondérée\n",
    "EEC_COV.columns = noms_var #on affiche le nom des variables correspondantes en x et y\n",
    "EEC_COV.set_index(EEC_COV.columns, inplace=True)\n",
    "EEC_COV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0eee1e-5da2-4b33-b909-8bdb59730a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser seaborn pour créer la heatmap\n",
    "sns.set(style=\"white\")\n",
    "sns.heatmap(EEC_COV, annot=False, cmap=\"coolwarm\", fmt=\".2f\", linewidths=.5)\n",
    "\n",
    "# Afficher la heatmap\n",
    "plt.title('Heatmap de la Matrice de Covariance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244e69f4-79d4-4592-92ac-f6c8fc837363",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## C - PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aa195a-891c-4338-b726-514a2b78241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la PCA\n",
    "pca = PCA()\n",
    "composantes_principales = pca.fit(EEC_COV).components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c96840-b4a6-4ab6-9ca7-0931631bd34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique des valeurs propres\n",
    "valeurs_propres = pca.explained_variance_ratio_\n",
    "plt.bar(range(1, len(valeurs_propres) + 1), valeurs_propres)\n",
    "plt.xlabel('Composantes principales')\n",
    "plt.ylabel('Variance expliquée')\n",
    "plt.title('Graphique des valeurs propres (Scree Plot)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec0676-2e89-40dc-bc8f-d37cafd49346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les deux premiers vecteurs propres\n",
    "first_pc = composantes_principales[:, 0]\n",
    "second_pc = composantes_principales[:, 1]\n",
    "variable_names = EEC_COV.columns.tolist()\n",
    "\n",
    "# Tracer le graphique de dispersion des deux premières composantes principales\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(first_pc, second_pc)\n",
    "\n",
    "# Annoter chaque point avec le nom de la variable\n",
    "for i, variable in enumerate(variable_names):\n",
    "    plt.annotate(f'Var {variable}', (first_pc[i], second_pc[i]), color='r', rotation=45)\n",
    "\n",
    "# Ajouter des labels et un titre au graphique\n",
    "plt.title('Contribution des variables aux deux premières composantes principales (Matrice de Covariance)')\n",
    "plt.xlabel('Première Composante Principale')\n",
    "plt.ylabel('Deuxième Composante Principale')\n",
    "\n",
    "# Afficher le graphique\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55078b90",
   "metadata": {},
   "source": [
    "# III - Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e908bc11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## A - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc90cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On choisit un ensemble de variables qui ne comprend que données sociodémographiques, géographiques associées à l'individu\n",
    "# et au ménage étudié et qui sont disponibles sur les deux années étudiées\n",
    "\n",
    "# Notre variable d'intérêt ici est ACTEU\n",
    "\n",
    "# Variables à inclure dans le modèle -> AGE3 ;  AGE5 ; ANNEE ;TRIM ;  CATAU2010R ; COURED ;\n",
    "# DIP11; ENFRED ; METRODOM ; NFRRED ; SEXE ; TYPMEN7 \n",
    "\n",
    "# Pour la prise en compte de la PCS, il n'est pas possible d'utiliser CSTOT /  CSTOTPRM / CSTOTR\n",
    "# Ce sont des variables qui distinguent les actifs des non-actifs\n",
    "# Possibilité de recoder à partir de CSP et CSA\n",
    "\n",
    "# Meme si les variables sont colinéaires (AGE3, AGE5), on peut les garder et faire en sorte que le predicteur\n",
    "# choisisse la plus pertinente\n",
    "\n",
    "list_var_selected = [\"ACTEU\",\"ANNEE\" ,\"TRIM\", \"AGE3\" ,  \"AGE5\"  , \"CATAU2010R\" ,\n",
    "\"COURED\" ,\"CSA\" ,\"CSP\" , \"DIP11\",\"ENFRED\" , \"METRODOM\" , \"NFRRED\" , \"SEXE\" , \"TYPMEN7\"]\n",
    "\n",
    "EEC_2019 = EEC_2019[list_var_selected]\n",
    "EEC_2020 = EEC_2020[list_var_selected] \n",
    "\n",
    "list_var = list(EEC_2019.columns.values)\n",
    "print(list_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae39d26",
   "metadata": {},
   "source": [
    "### Recodage d'une variable de PCS à deux niveaux regroupant actifs occupés et les autres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c51d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On commence par vérifier que les variables CSP (actifs occupés) et CSA (non actifs occupés) ne sont pas renseignées pour les mêmes individus\n",
    "\n",
    "non_nan_csp = EEC_2019['CSP'].notna()\n",
    "non_nan_csa = EEC_2019['CSA'].notna()\n",
    "\n",
    "# Vérifiez si les variables sont renseignées pour les mêmes individus\n",
    "positions_differentes = (non_nan_csp != non_nan_csa).any()\n",
    "\n",
    "# Affichez le résultat\n",
    "if positions_differentes:\n",
    "    print(\"Les variables ne sont pas renseignées pour les mêmes individus\")\n",
    "else:\n",
    "    print(\"Les variables sont renseignées pour les mêmes individus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2a4a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#On peut donc sommer les deux colonnes pour obtenir la valeur voulue\n",
    "EEC_2019['PCS'] = EEC_2019['CSA'].add(EEC_2019['CSP'], fill_value=0)\n",
    "\n",
    "pcs_manquante = EEC_2019['PCS'].isna().sum()\n",
    "\n",
    "print(pcs_manquante)\n",
    "#1260 valeurs manquantes, donc assez peu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acaa6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idem pour EEC_2020\n",
    "#Création d'une variable de PCS avec la PCS des actifs occupés et l'ancienne des inactifs et chômeurs.\n",
    "non_nan_csp = EEC_2020['CSP'].notna()\n",
    "non_nan_csa = EEC_2020['CSA'].notna()\n",
    "\n",
    "# Vérifiez si les variables sont renseignées pour les mêmes individus\n",
    "positions_differentes = (non_nan_csp != non_nan_csa).any()\n",
    "\n",
    "# Affichez le résultat\n",
    "if positions_differentes:\n",
    "    print(\"Les variables ne sont pas renseignées pour les mêmes individus\")\n",
    "else:\n",
    "    print(\"Les variables sont renseignées pour les mêmes individus\")\n",
    "\n",
    "#On peut donc sommer les deux colonnes\n",
    "EEC_2020['PCS'] = EEC_2020['CSA'].add(EEC_2020['CSP'], fill_value=0)\n",
    "\n",
    "pcs_manquante = EEC_2020['PCS'].isna().sum()\n",
    "\n",
    "print(pcs_manquante)\n",
    "#864 valeurs manquantes, donc très peu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc6fee0-8081-4c99-a0fe-78cbd90aa7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention, notre variable 'PCS' contient une modalité en 2019,qui n'existe pas en 2020\n",
    "#Cela vient de la modalité \"10\" de la PCS (\"agriculteur\") qui n'existe pas pour 2020, et uniquement chez des non actifs occupés\n",
    "#On peut supposer qu'il s'agit d'agriculteurs pour lesquels la taille d'exploitation n'a pu être déterminée...\n",
    "#Cela ne concerne que deux observations\n",
    "EEC_2019['PCS'].value_counts()[0]\n",
    "print(EEC_2019.shape)\n",
    "#Lors de la conversion de la variable en indicatrice, nous allons donc obtenir des bases de tailles différentes\n",
    "#pour éviter cela, on décide de supprimer ces deux observations\n",
    "EEC_2019 = EEC_2019[EEC_2019['PCS'] != 10]\n",
    "print(EEC_2019.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On enlève de la base les variables CSA et CSP\n",
    "EEC_2019 = EEC_2019.drop(['CSA', 'CSP'], axis=1)\n",
    "EEC_2020 = EEC_2020.drop(['CSA', 'CSP'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfca855",
   "metadata": {},
   "source": [
    "### Analyses des NA de notre base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae9d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comptons le nombre de NaN dans nos df\n",
    "nombre_lignes_nan_2019 = EEC_2019.isnull().any(axis=1).sum()\n",
    "nombre_lignes_nan_2020 = EEC_2020.isnull().any(axis=1).sum()\n",
    "print(nombre_lignes_nan_2019)\n",
    "print(nombre_lignes_nan_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3ed031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolons nos observations avec des NAN\n",
    "EEC_2019_with_nulls = EEC_2019[EEC_2019.isna().any(axis=1)]\n",
    "EEC_2020_with_nulls = EEC_2020[EEC_2020.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5497d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EEC_2019_with_nulls.shape)\n",
    "print(EEC_2020_with_nulls.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "EEC_2019_with_nulls = EEC_2019_with_nulls.copy()\n",
    "EEC_2019_with_nulls['total_nan'] = EEC_2019_with_nulls.isna().sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a9df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EEC_2019_with_nulls['total_nan'].mean())\n",
    "#En moyenne, 1,79 valeurs manquantes, sur 14 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b026c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On affiche un tableau avec le nombre de NAN par variable, et le pourcentage des observations manquantes que cela représente\n",
    "nan_count_variable_2019 = EEC_2019_with_nulls.isna().sum()\n",
    "print(nan_count_variable_2019)\n",
    "nan_percentage_variable_2019 = (EEC_2019_with_nulls.isna().sum() / len(EEC_2019_with_nulls)) * 100\n",
    "print(nan_percentage_variable_2019)\n",
    "#A chaque fois il nous manque donc le diplôme, et dans environ un tiers des cas la PCS recodée et la variable d'intérêt, un cinquième des nationalités"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fdb014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Même chose pour 2020\n",
    "EEC_2020_with_nulls = EEC_2020_with_nulls.copy()\n",
    "EEC_2020_with_nulls['total_nan'] = EEC_2020_with_nulls.isna().sum(axis=1)\n",
    "print(EEC_2020_with_nulls['total_nan'].mean())\n",
    "#En moyenne, 1,67 valeurs manquantes, sur 14 variables\n",
    "nan_count_variable_2020 = EEC_2020_with_nulls.isna().sum()\n",
    "print(nan_count_variable_2020)\n",
    "nan_percentage_variable_2020 = (EEC_2020_with_nulls.isna().sum() / len(EEC_2020_with_nulls)) * 100\n",
    "print(nan_percentage_variable_2020)\n",
    "#On retrouve des choses très semblables sur 2020, avec encore une fois surtout le diplôme qui manque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13572ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commençons par supprimer les observations pour lesquelles notre variable d'intérêt n'est pas renseignée --> inutilisables\n",
    "EEC_2019_with_nulls_non_ACTEU = EEC_2019_with_nulls.dropna(subset=['ACTEU'])\n",
    "EEC_2019_with_nulls_non_ACTEU = EEC_2019_with_nulls_non_ACTEU.copy()\n",
    "EEC_2019_with_nulls_non_ACTEU['total_nan'] = EEC_2019_with_nulls_non_ACTEU.isna().sum(axis=1)\n",
    "print(EEC_2019_with_nulls_non_ACTEU['total_nan'].mean())\n",
    "#En moyenne, 1,01 valeurs manquantes, sur 13 variables\n",
    "nan_count_variable_2019_non_ACTEU = EEC_2019_with_nulls_non_ACTEU.isna().sum()\n",
    "print(nan_count_variable_2019_non_ACTEU)\n",
    "nan_percentage_variable_2019_non_ACTEU = (EEC_2019_with_nulls_non_ACTEU.isna().sum() / len(EEC_2019_with_nulls_non_ACTEU)) * 100\n",
    "print(nan_percentage_variable_2019_non_ACTEU)\n",
    "#A part une poignée de nationalités, il ne manque plus que le diplôme, pour tout le monde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84794012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commençons par supprimer les observations pour lesquelles notre variable d'intérêt n'est pas renseignée --> inutilisables\n",
    "EEC_2020_with_nulls_non_ACTEU = EEC_2020_with_nulls.dropna(subset=['ACTEU'])\n",
    "EEC_2020_with_nulls_non_ACTEU = EEC_2020_with_nulls_non_ACTEU.copy()\n",
    "EEC_2020_with_nulls_non_ACTEU['total_nan'] = EEC_2020_with_nulls_non_ACTEU.isna().sum(axis=1)\n",
    "print(EEC_2020_with_nulls_non_ACTEU['total_nan'].mean())\n",
    "#En moyenne, 1,01 valeurs manquantes, sur 13 variables\n",
    "nan_count_variable_2020_non_ACTEU = EEC_2020_with_nulls_non_ACTEU.isna().sum()\n",
    "print(nan_count_variable_2020_non_ACTEU)\n",
    "nan_percentage_variable_2020_non_ACTEU = (EEC_2020_with_nulls_non_ACTEU.isna().sum() / len(EEC_2020_with_nulls_non_ACTEU)) * 100\n",
    "print(nan_percentage_variable_2020_non_ACTEU)\n",
    "#A part une poignée de nationalités, il ne manque plus que le diplôme, pour tout le monde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27221af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Il y a très peu de valeurs manquantes dans les variables, 4205 variables concernées en 2019, 3379 en 2020.\n",
    "#Si on supprime d'office les observations pour lesquelles manquent la variable d'intérêt, on tombe à 2945 et 2515, et c'est systématiquement le diplôme qui manque\n",
    "#On choisit donc de les supprimer de la base pour le moment\n",
    "print(EEC_2019.shape)\n",
    "print(EEC_2020.shape)\n",
    "EEC_2019 = EEC_2019.dropna() \n",
    "EEC_2020 = EEC_2020.dropna() \n",
    "print(EEC_2019.shape)\n",
    "print(EEC_2020.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3317cc1b",
   "metadata": {},
   "source": [
    "### Suite et fin du préprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0daedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de l'ensemble des variables catégorielles en indicatrices\n",
    "EEC_2019 = pd.get_dummies(EEC_2019, columns=[\"AGE3\" ,  \"AGE5\"  , \"CATAU2010R\" ,\n",
    "\"PCS\", \"DIP11\", \"NFRRED\" , \"TYPMEN7\"])\n",
    "EEC_2020 = pd.get_dummies(EEC_2020, columns=[\"AGE3\" ,  \"AGE5\"  , \"CATAU2010R\" ,\n",
    "\"PCS\" , \"DIP11\", \"NFRRED\" , \"TYPMEN7\"])\n",
    "\n",
    "#RECODAGE des variables binaires\n",
    "\n",
    "EEC_2019['EMPLOI'] = EEC_2019['ACTEU'].apply(lambda x: x == 1)\n",
    "EEC_2020['EMPLOI'] = EEC_2020['ACTEU'].apply(lambda x: x == 1)\n",
    "\n",
    "EEC_2019['ACTIF'] = EEC_2019['ACTEU'].apply(lambda x: (x == 1) or (x == 2))\n",
    "EEC_2020['ACTIF'] = EEC_2020['ACTEU'].apply(lambda x: (x == 1) or (x == 2))\n",
    "\n",
    "EEC_2019['FEMME'] = EEC_2019['SEXE'].apply(lambda x: x == 2)\n",
    "EEC_2020['FEMME'] = EEC_2020['SEXE'].apply(lambda x: x == 2)\n",
    "\n",
    "EEC_2019['COUPLE'] = EEC_2019['COURED'].apply(lambda x: x == 2)\n",
    "EEC_2020['COUPLE'] = EEC_2020['COURED'].apply(lambda x: x == 2)\n",
    "\n",
    "EEC_2019['ENFANT'] = EEC_2019['ENFRED'].apply(lambda x: x == 2)\n",
    "EEC_2020['ENFANT'] = EEC_2020['ENFRED'].apply(lambda x: x == 2)\n",
    "\n",
    "\n",
    "EEC_2019['DOM'] = EEC_2019['METRODOM'].apply(lambda x: x == 2)\n",
    "EEC_2020['DOM'] = EEC_2020['METRODOM'].apply(lambda x: x == 2)\n",
    "\n",
    "#On retire les variables non recodées\n",
    "EEC_2019 = EEC_2019.drop(['METRODOM', 'ENFRED' , 'COURED', 'SEXE',\"ACTEU\"], axis=1)\n",
    "EEC_2020 = EEC_2020.drop(['METRODOM', 'ENFRED' , 'COURED', 'SEXE',\"ACTEU\"], axis=1)\n",
    "\n",
    "# 73 variables\n",
    "print(EEC_2019.shape)\n",
    "print(EEC_2020.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5abad6-93ff-4af8-ad1a-c338d4e05661",
   "metadata": {},
   "source": [
    "#### Création de notre sous-ensemble, et de nos array pour sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb2351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour rendre les calculs plus rapide, on se concentre sur un sous-ensemble\n",
    "EEC_2019_subsample = EEC_2019.sample(n=10000, random_state=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86693e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balanced sample avec prediction emploi\n",
    "proportion_values = EEC_2019_subsample['EMPLOI'].value_counts(normalize=True)\n",
    "print(proportion_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73abf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description d'une feature pour check booléen\n",
    "variable_description = EEC_2019_subsample[\"FEMME\"].describe()\n",
    "\n",
    "print(variable_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b8b718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# On transforme nos dataframes en array car sklearn est bien adapté pour travailler sur ce type de données\n",
    "\n",
    "# Array contentant l'ensemble des variables explicatives (on exclut donc CHOMAGE/ANNEE/TRIM)\n",
    "X = np.array(EEC_2019_subsample.drop(columns=[\"ACTIF\",\"EMPLOI\",\"ANNEE\", \"TRIM\"]))\n",
    "print(X.shape)\n",
    "\n",
    "# Array contentant la variable expliquée\n",
    "y = np.array(EEC_2019_subsample[\"EMPLOI\"])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#création de nos ensemble de train et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db77f2b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## B - Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f24aae-4cb6-48f6-9896-8b05a8841f4f",
   "metadata": {},
   "source": [
    "Nous essayons ici d'appliquer un modèle de type SVM pour prédire la situation d'emploi à partir des variables socio-démographiques, en essayant de sélectionner le paramètre régularisation qui permet la meilleure précision sur notre (sous)-échantillon de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ee874",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_test = list(), list()\n",
    "f1_train, f1_test = [], []\n",
    "\n",
    "C_range = np.linspace(0.1, 20, 50)\n",
    "for param in C_range:\n",
    "    clf = SVC( C=param, random_state=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc_train.append(clf.score(X_train, y_train))\n",
    "    acc_test.append(clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    f1_train.append(f1_score(y_train, y_pred_train, average='binary'))  \n",
    "    \n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    f1_test.append(f1_score(y_test, y_pred_test, average='binary'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(C_range, f1_train, label='F1 score train', lw=6)\n",
    "plt.plot(C_range, f1_test, label='F1 score test', lw=6)\n",
    "\n",
    "plt.plot(C_range, acc_train, label='Accuracy train ', lw=6)\n",
    "plt.plot(C_range, acc_test, label='Accuracy test', lw=6)\n",
    "\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "plt.xlabel(\"C\", fontweight=\"bold\", fontsize=20)\n",
    "plt.ylabel(\"Performance\", fontweight=\"bold\", fontsize=20)\n",
    "plt.xticks(fontweight=\"bold\", fontsize=15)\n",
    "plt.yticks(fontweight=\"bold\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71ac281-58ea-4550-9319-16786a296ac0",
   "metadata": {},
   "source": [
    "Il semble ici que très vite la performance diminue sur le test, dès des valeurs assez petites du paramètre C, ce qui indique que le modèle de SVM rentre très vite dans une phase de surapprentissage : on s'attends à ce que notre modèle ne sélectionne qu'un nomre limité de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilisation de GridSearch pour trouver le meilleur paramètre de régularisation\n",
    "params= { 'C':np.linspace(0.001, 5, 50) }\n",
    "gs = GridSearchCV(estimator=SVC( C=params, random_state=3), \n",
    "                   param_grid=params,\n",
    "                   cv=5)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "#Calcule accuracy du meilleur model trouvé par grid search sur le sample choisi\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_train, y_train))\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ccab5c-c758-4ad1-9135-f56f615feb58",
   "metadata": {},
   "source": [
    "Comme attendu, GridSearch nous propose donc un paramètre de régularisation faible, autours de 2,85, avec un score de 0,896, signifiant que 89,6% de nos observations du test sont correctement prédites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC( C=gs.best_params_['C'], random_state=3)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "cm= confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(\n",
    "            confusion_matrix=cm,\n",
    "            display_labels=clf.classes_\n",
    "       )\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97acc4ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## C - Regression logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4c1e6-fae3-4ef7-a3fc-a9b37a0e7fd2",
   "metadata": {},
   "source": [
    "Nous essayons ici une autre méthode, à savoir la régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e0cddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train, acc_test = list(), list()\n",
    "f1_train, f1_test = [], []\n",
    "\n",
    "                        \n",
    "C_range = np.linspace(0.1, 20, 50)\n",
    "\n",
    "for param in C_range:\n",
    "    clf = LogisticRegression(C=param,random_state=3, penalty=\"l1\",solver='liblinear' )\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc_train.append(clf.score(X_train, y_train))\n",
    "    acc_test.append(clf.score(X_test, y_test))\n",
    "    \n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    f1_train.append(f1_score(y_train, y_pred_train, average='binary'))  \n",
    "    \n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    f1_test.append(f1_score(y_test, y_pred_test, average='binary'))\n",
    "    \n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.plot(C_range, f1_train, label='F1 score train', lw=6)\n",
    "plt.plot(C_range, f1_test, label='F1 score test', lw=6)\n",
    "\n",
    "plt.plot(C_range, acc_train, label='Accuracy train ', lw=6)\n",
    "plt.plot(C_range, acc_test, label='Accuracy test', lw=6)\n",
    "\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "plt.xlabel(\"C\", fontweight=\"bold\", fontsize=20)\n",
    "plt.ylabel(\"Performance\", fontweight=\"bold\", fontsize=20)\n",
    "plt.xticks(fontweight=\"bold\", fontsize=15)\n",
    "plt.yticks(fontweight=\"bold\", fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#TOUJOURS GRAPH UN PEU BIZARRE -> PQ PERFORMANCE MEILLEURE SUR TEST QUE SUR TRAIN??? \n",
    "#Ce problème disparait dès lors que l'on se place avec uneautre répartition train/test (j'ai changé la seed)\n",
    "# Pas de diminution de la performance quand regularisation diminue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e751c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processus de cross-validation qui découpe notre échantillon en 5 et détermine le paramètre de régularisation optimal\n",
    "# ATTENTION -> gain de selection des paramètres via cross validation est marginal par rapport à la selection des bonnes variables \n",
    "# On utilise GridSearch ici parce qu'on veut combiner plusieurs paramètres\n",
    "\n",
    "#  By default, scikit-learn's GridSearchCV does not select the most parsimonious model (fewer features)\n",
    "# when multiple models have equal scores. GridSearchCV uses the mean cross-validated score to select\n",
    "# the best model based on the specified scoring metric.\n",
    "# If there are multiple models with the same highest mean score, GridSearchCV selects\n",
    "# the one with the smallest value of the parameters.\n",
    "\n",
    "# If we want to select the most parsimonious model among those with equal scores,\n",
    "# you may need to implement a custom scoring function\n",
    "\n",
    "params= { 'C':np.linspace(0.001, 5, 50) }\n",
    "gs = GridSearchCV(estimator=LogisticRegression(random_state=3, penalty =\"l1\",  solver='liblinear'), \n",
    "                   param_grid=params,\n",
    "                   cv=5)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "results_df = pd.DataFrame(gs.cv_results_)\n",
    "print(results_df[['params', 'mean_test_score', 'std_test_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda6e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On refait meme process en jouant en + sur le critère de penalty et le tol\n",
    "# tol = tolerance for stopping criteria. It defines the stopping criterion for the solver to converge to a solution.\n",
    "\n",
    "params= { 'C':np.linspace(0.001, 5, 50),'tol': [0.01, 0.1, 1, 10] }\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=LogisticRegression(random_state=3, penalty =\"l1\",  solver='liblinear'), \n",
    "                   param_grid=params,\n",
    "                   cv=5)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# calcule accuracy du meilleur model trouvé par grid search sur le sample choisi\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_train, y_train))\n",
    "print(gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490938fe-4230-4505-8414-d702522cf2e4",
   "metadata": {},
   "source": [
    "Le modèle choisit atteint une précision de l'ordre de 86,5% sur le test, ce qui est légérement moins efficace que le modèle SVMde la partie précédente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e7d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression( tol=gs.best_params_['tol'], C=gs.best_params_['C'], random_state=3,\n",
    "                         penalty =\"l1\", solver='liblinear')\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "cm= confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(\n",
    "            confusion_matrix=cm,\n",
    "            display_labels=clf.classes_\n",
    "       )\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4c4977",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_variables = np.sum(np.abs(clf.coef_) > 1e-10)\n",
    "print(\"Number of variables considered:\", num_variables)\n",
    "#Conserve la grande majorité des variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471cf014",
   "metadata": {},
   "source": [
    "## D - Selections de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd54fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Lasso linear model with cross-validated selection of the regularization parameter (alpha) to find the optimal\n",
    "clf = LassoCV()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Je m'impose un maximum de 10 features selectionnées\n",
    "sfm = SelectFromModel(clf, prefit=False, max_features=10)\n",
    "\n",
    "# X transform contient un nombre réduit de caractéristiques basé sur la sélection\n",
    "# des caractéristiques par régression Lasso\n",
    "sfm.fit(X,y)\n",
    "X_transform = sfm.transform(X)\n",
    "print(\"Transformed data shape:\", X_transform.shape)\n",
    "print(\"Original data shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac1d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_transform, \n",
    "                                                    y, \n",
    "                                                    test_size=.2, random_state=4)\n",
    "\n",
    "\n",
    "params= { 'C':np.linspace(0.001, 1, 50), 'tol': [0.01, 0.1, 1, 10] }\n",
    "\n",
    "\n",
    "\n",
    "gs = GridSearchCV(estimator=LogisticRegression( random_state=3, penalty = \"l1\", solver='liblinear'), \n",
    "                   param_grid=params,\n",
    "                   cv=5)\n",
    "\n",
    "gs.fit(X_train_2, y_train_2)\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(gs.score(X_train_2, y_train_2))\n",
    "print(gs.score(X_test_2, y_test_2))\n",
    "\n",
    "#Perd un peu en performance avec 10 variables max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ff3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(tol=gs.best_params_['tol'], C=gs.best_params_['C'],\n",
    "                         random_state=3, penalty=\"l1\", solver='liblinear')\n",
    "\n",
    "clf.fit(X_train_2, y_train_2)\n",
    "y_pred_2 = clf.predict(X_test_2)\n",
    "\n",
    "cm= confusion_matrix(y_test_2, y_pred_2)\n",
    "disp = ConfusionMatrixDisplay(\n",
    "            confusion_matrix=cm,\n",
    "            display_labels=clf.classes_\n",
    "       )\n",
    "disp.plot()\n",
    "\n",
    "num_variables = np.sum(np.abs(clf.coef_) > 1e-10)\n",
    "print(\"Number of variables considered:\", num_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db91fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify the indices of the variables used for prediction in X_transform\n",
    "variables_indices = np.where(np.abs(clf.coef_) > 1e-10)[1]\n",
    "\n",
    "X_transform_useful_variables = X_transform[:,variables_indices]\n",
    "\n",
    "# X_transform n'a pas les meme indices pour les colonnes que X ou EEC_2019 -> Je verifie quelle colonne a les meme \n",
    "# valeurs\n",
    "for j in range(X_transform_useful_variables.shape[1]):\n",
    "    for column_name in EEC_2019_subsample.columns:\n",
    "        # Check if the corresponding column in X_transform_bis has equal values\n",
    "        is_equal = np.all(X_transform_useful_variables[:, j] == EEC_2019_subsample[column_name].values)\n",
    "        \n",
    "        if is_equal:\n",
    "            print(f\"The {j+1} column  is equal to the {column_name} column in EEC_2019_subsample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77851951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# J'essaie de reconstruire la variable predite à partir des variables et des coefs associés\n",
    "linear_pred_3 = clf.intercept_ + np.dot(X_test_2[:, variables_indices], clf.coef_[:, variables_indices].T) \n",
    "y_pred_3 = 1 / (1 + np.exp(-linear_pred_3))\n",
    "threshold = 0.5  \n",
    "y_pred_3_binary = (y_pred_3 > threshold).astype(bool).ravel()\n",
    "\n",
    "comparison = np.equal(y_pred_2, y_pred_3_binary)\n",
    "equal_predictions = np.sum(comparison)\n",
    "percentage_equal = (equal_predictions / len(y_pred_2)) * 100 \n",
    "print(\"% same predictor\",percentage_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b5e0c0",
   "metadata": {},
   "source": [
    "## E - Prediction actifs/inactifs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26bbd1",
   "metadata": {},
   "source": [
    "Nous pensons qu'une partie du problème de prédiction réside dans la distinction entre certains chômeurs de courte durée et actifs occupés. Afin de vérifier cela,  nous construisons un prédicteur du fait d'être actif et comparons la qualité de la prediction.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bf0fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array contentant la variable expliquée\n",
    "y_bis = np.array(EEC_2019_subsample[\"ACTIF\"])\n",
    "print(y_bis.shape)\n",
    "\n",
    "print(y_bis[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b06299",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_transform)\n",
    "\n",
    "cm= confusion_matrix(y_bis,y_pred)\n",
    "disp = ConfusionMatrixDisplay(\n",
    "            confusion_matrix=cm,\n",
    "            display_labels=clf.classes_\n",
    "       )\n",
    "disp.plot()\n",
    "\n",
    "# Prédicteur de l'emploi déjà très bon pour prédire actif/pas actif => logique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71587919",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Lasso linear model with cross-validated selection of the regularization parameter (alpha) to find the optimal\n",
    "clf = LassoCV()\n",
    "clf.fit(X, y_bis)\n",
    "\n",
    "# Je m'impose un maximum de 10 features selectionnées\n",
    "sfm = SelectFromModel(clf, prefit=False, max_features=10)\n",
    "\n",
    "sfm.fit(X,y_bis)\n",
    "\n",
    "X_transform_bis = sfm.transform(X)\n",
    "print(X_transform_bis.shape)\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_transform_bis, \n",
    "                                                    y_bis, \n",
    "                                                    test_size=.2, random_state=3)\n",
    "\n",
    "\n",
    "params= { 'C':np.linspace(0.001, 1, 50), 'tol': [0.01, 0.1, 1, 10] }\n",
    "\n",
    "gs = GridSearchCV(estimator=LogisticRegression( random_state=3, penalty = \"l1\", solver='liblinear'), \n",
    "                   param_grid=params,\n",
    "                   cv=5)\n",
    "\n",
    "gs.fit(X_train_2, y_train_2)\n",
    "\n",
    "clf = LogisticRegression(tol=gs.best_params_['tol'], C=gs.best_params_['C'],\n",
    "                         random_state=3, penalty=\"l1\", solver='liblinear')\n",
    "\n",
    "clf.fit(X_train_2, y_train_2)\n",
    "y_pred_2 = clf.predict(X_test_2)\n",
    "\n",
    "cm= confusion_matrix(y_test_2, y_pred_2)\n",
    "disp = ConfusionMatrixDisplay(\n",
    "            confusion_matrix=cm,\n",
    "            display_labels=clf.classes_\n",
    "       )\n",
    "disp.plot()\n",
    "\n",
    "print(clf.coef_)\n",
    "num_variables = np.sum(np.abs(clf.coef_) > 1e-10)\n",
    "print(\"Number of variables considered:\", num_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812c817f-de68-43e3-825d-bd82eb265658",
   "metadata": {},
   "source": [
    "La qualité de la prediction est meilleure puisqu'on passe de 269 (197 + 72) erreurs de prédictions de l'emploi à 209 (117 + 92) erreurs de prédictions du statut actif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7835b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=> ON dirait que les indices des colonnes change lorsqu'on passe de X à X_transform\n",
    "# => ['AGE3_15' 'AGE3_30' 'AGE3_50'] ne sont pas les \"bonnes\" variables\n",
    "# Iterate over a range of columns in X_transform_bis\n",
    "for j in range(X_transform_bis.shape[1]):\n",
    "    for column_name in EEC_2019_subsample.columns:\n",
    "        # Check if the corresponding column in X_transform_bis has equal values\n",
    "        is_equal = np.all(X_transform_bis[:, j] == EEC_2019_subsample[column_name].values)\n",
    "        \n",
    "        if is_equal:\n",
    "            print(f\"The {j+1} column in X_transform_bis is equal to the {column_name} column in EEC_2019_subsample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b42e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_4 = clf.predict(X_transform_bis)\n",
    "\n",
    "cm= confusion_matrix(y_bis,y_pred_4)\n",
    "disp = ConfusionMatrixDisplay(\n",
    "            confusion_matrix=cm,\n",
    "            display_labels=clf.classes_\n",
    "       )\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4c1d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2020 = np.array(EEC_2020.drop(columns=[\"ACTIF\",\"EMPLOI\",\"ANNEE\", \"TRIM\"]))\n",
    "y_2020 = np.array(EEC_2020[\"ACTIF\"])\n",
    "\n",
    "X_2020_transform = sfm.transform(X_2020)\n",
    "print(X_2020_transform.shape)\n",
    "\n",
    "y_pred_2020 = clf.predict(X_2020_transform)\n",
    "\n",
    "\n",
    "cm= confusion_matrix(y_2020,y_pred_2020)\n",
    "disp = ConfusionMatrixDisplay(\n",
    "            confusion_matrix=cm,\n",
    "            display_labels=clf.classes_\n",
    "       )\n",
    "disp.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
